{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNdKYNwXI+mzUfMtetrg+ld",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SebsPER/TDP-Sprint3/blob/main/Const_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i1rZGDDjdwd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4432623b-86ce-4a9c-b4f3-cdf19b99b1b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.19.4\n",
            "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 19.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.4) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.4) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.4) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.4) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 72.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.4) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.4) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.4) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.4) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.4) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.4) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.4) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.4) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.4) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.12.1 transformers-4.19.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 62.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from evaluate) (4.13.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from evaluate) (2.23.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 68.4 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from evaluate) (1.21.6)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from evaluate) (2022.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from evaluate) (21.3)\n",
            "Collecting datasets>=2.0.0\n",
            "  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n",
            "\u001b[K     |████████████████████████████████| 441 kB 42.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from evaluate) (4.64.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from evaluate) (1.3.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from evaluate) (0.10.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->evaluate) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->evaluate) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->evaluate) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->evaluate) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->evaluate) (2022.9.24)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 70.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->evaluate) (3.10.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 72.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->evaluate) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\n",
            "Installing collected packages: urllib3, dill, xxhash, responses, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "Successfully installed datasets-2.6.1 dill-0.3.5.1 evaluate-0.3.0 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.10.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.12.1)\n"
          ]
        }
      ],
      "source": [
        "#!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install transformers==4.19.4\n",
        "!pip install evaluate\n",
        "!pip install datasets\n",
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create folders:\n",
        "\n",
        "- tokenizer\n",
        "\n",
        "- ddGPT\n",
        "\n",
        "- ddGptModel"
      ],
      "metadata": {
        "id": "vuU1Njzw1zoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, DataCollatorForLanguageModeling, Trainer, TrainingArguments, PhrasalConstraint, AutoTokenizer, AutoModelForCausalLM\n",
        "#from tensorflow.keras.optimizers import Adam\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "pQ1l88brCtHb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=\"dd.txt\", vocab_size=52_000, min_frequency=2, special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"]) #\n",
        "\n",
        "# Save files to disk\n",
        "#tokenizer.save_model(\".\", \"ddGPT2\")\n",
        "tokenizer.save_model(\"tokenizer\")"
      ],
      "metadata": {
        "id": "25mrbm4Zff0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e50d67a-1d4e-4120-cd7c-fe479b5a1b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tokenizer/vocab.json', 'tokenizer/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "#tokenizer = GPT2Tokenizer('ddGPT2-vocab.json', 'ddGPT2-merges.txt')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('tokenizer')\n",
        "\n",
        "#if tokenizer.pad_token is None:\n",
        "#    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "tokenizer.add_special_tokens({\n",
        "    \"eos_token\": \"</s>\",\n",
        "    \"bos_token\": \"<s>\",\n",
        "    \"unk_token\": \"<unk>\",\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"mask_token\": \"<mask>\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP1ETBwmWrNn",
        "outputId": "64737947-760b-42e2-c00d-18d63fd58c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    bos_token=tokenizer.bos_token_id,\n",
        "    eos_token=tokenizer.eos_token_id,\n",
        ")\n",
        "#model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel(config)\n",
        "\n",
        "dataset = load_dataset(\"text\", data_files=\"dd.txt\")"
      ],
      "metadata": {
        "id": "07vqunjaZn42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0])\n",
        "\n",
        "def encode(lines):\n",
        "    return tokenizer(lines['text'], truncation=True, add_special_tokens=True, ) #, max_length = 100\n",
        "\n",
        "dataset.set_transform(encode)\n",
        "\n",
        "print(dataset['train'][0])\n",
        "print(\"len\",len(dataset['train']))\n",
        "dataset = dataset['train']\n",
        "\n",
        "t = \"hello world\"\n",
        "en = tokenizer.encode(t)\n",
        "\n",
        "print(en)\n",
        "print(tokenizer.decode(en))\n",
        "#dataset = dataset['train']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-2pzOpDlKWO",
        "outputId": "fa5cf95a-b7e8-4575-db48-dde11480cc89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Vistaru, Lord of the Mountain, is attacking small towns surrounded by his kingdom in hopes of expanding his army before his assault on the capital.'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [58, 605, 276, 89, 16, 3176, 285, 265, 4857, 16, 299, 1365, 763, 3091, 2786, 384, 439, 1469, 297, 3268, 285, 3860, 274, 439, 1686, 714, 439, 1055, 69, 645, 334, 265, 5101, 18], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "len 1061\n",
            "[263, 291, 83, 846]\n",
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataCollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
      ],
      "metadata": {
        "id": "nA53FPlqsKkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"ddGPT\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=300,\n",
        "    #per_gpu_train_batch_size=64,\n",
        "    per_device_train_batch_size=20,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=dataCollator\n",
        ")"
      ],
      "metadata": {
        "id": "hrIx9wOLwtI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Php6flrDzxt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"ddGptModel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCdoEDGUz1KT",
        "outputId": "33f694ad-a3ad-4f3c-9639-9dee66a03343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ddGptModel\n",
            "Configuration saved in ddGptModel/config.json\n",
            "Model weights saved in ddGptModel/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "tNMhKLk-_O5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('tokenizer')\n",
        "tokenizer.add_special_tokens({\n",
        "    \"eos_token\": \"</s>\",\n",
        "    \"bos_token\": \"<s>\",\n",
        "    \"unk_token\": \"<unk>\",\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"mask_token\": \"<mask>\"\n",
        "})"
      ],
      "metadata": {
        "id": "cj6v-7OWUWsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelT = GPT2LMHeadModel.from_pretrained(\"ddGptModel\")"
      ],
      "metadata": {
        "id": "JngkAo-CLvcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"KoboldAI/GPT-Neo-1.3B-Adventure\")\n",
        "\n",
        "force_word = \"scared\"\n",
        "force_flexible = [\"scream\", \"screams\", \"screaming\", \"screamed\"]\n",
        "\n",
        "force_words_ids = [\n",
        "    tokenizer([force_word], add_special_tokens=False).input_ids, #, return_tensors=\"pt\",\n",
        "    #tokenizer(force_flexible, padding=True, add_prefix_space=True, add_special_tokens=False).input_ids, #truncation= True, \n",
        "]\n",
        "\n",
        "generator = pipeline('text-generation', \n",
        "                     model='KoboldAI/GPT-Neo-1.3B-Adventure',\n",
        "                     num_beams=3)"
      ],
      "metadata": {
        "id": "emOwpLFqsiNs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\"> The party\", do_sample=False, min_length=50, force_words_ids=force_words_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3wWZgLrujOx",
        "outputId": "cb46c4c5-e4a7-4b43-eed9-27336563ba6a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': '> The party is over.scared.\\n\\n> You go back to your room.\\nYou go back to your room.\\n\\nYou go back to your room.\\n\\nYou go back to your room.\\n\\nYou go back'}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\"> The party\", do_sample=True, min_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnSbV8YoPqVR",
        "outputId": "fd7e306c-3067-4a26-f853-6a2f75596632"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"> The party is in a field somewhere. You're looking for a couple of your friends.'\\n'Hey, no need to be rude to me.'\\n'But it's rude to leave these people behind! I'm going to enjoy this,\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"KoboldAI/GPT-Neo-1.3B-Adventure\")\n",
        "\n",
        "modelT = AutoModelForCausalLM.from_pretrained(\"KoboldAI/GPT-Neo-1.3B-Adventure\")"
      ],
      "metadata": {
        "id": "84EZCL4TvEC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "force_word = \"scared\"\n",
        "force_flexible = [\"scream\", \"screams\", \"screaming\", \"screamed\"]\n",
        "\n",
        "force_words_ids = [\n",
        "    tokenizer([force_word], add_prefix_space=True, add_special_tokens=False).input_ids, #, return_tensors=\"pt\",\n",
        "    #tokenizer(force_flexible, padding=True, add_prefix_space=True, add_special_tokens=False).input_ids, #truncation= True, \n",
        "]\n",
        "\n",
        "starting_text = [\"A mage\", \"A kingdom\", 'The party']\n",
        "\n",
        "input_ids = tokenizer(starting_text, return_tensors=\"pt\").input_ids\n",
        "print(\"INPUT:\",input_ids)\n",
        "\n",
        "outputs = modelT.generate(\n",
        "    input_ids,\n",
        "    #force_words_ids=force_words_ids,\n",
        "    num_beams=3,\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=1,\n",
        "    remove_invalid_values=True,\n",
        "    max_length=50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "print(tokenizer.decode(outputs[1], skip_special_tokens=True))\n",
        "print(tokenizer.decode(outputs[2], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "UERuK-vBfzPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phrasal Constraint"
      ],
      "metadata": {
        "id": "nq5NB140_dbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "force_word = \"scared\"\n",
        "force_flexible = [\"scream\", \"screams\", \"screaming\", \"screamed\"]\n",
        "force_words = [\" attack\", \" dragon\"]\n",
        "\n",
        "constraints = []\n",
        "for constraint_entitiy in force_words:\n",
        "    constraints.append(PhrasalConstraint(tokenizer(constraint_entitiy).input_ids))\n",
        "\n",
        "starting_text = [\"A mage\", \"A kingdom\"]\n",
        "\n",
        "input_ids = tokenizer(starting_text, return_tensors=\"pt\").input_ids\n",
        "print(\"INPUT:\",input_ids)\n",
        "\n",
        "outputs = modelT.generate(\n",
        "    input_ids,\n",
        "    #constraints=constraints,\n",
        "    #force_words_ids=force_words_ids,\n",
        "    num_beams=10,\n",
        "    #num_return_sequences=1,\n",
        "    #no_repeat_ngram_size=1,\n",
        "    #remove_invalid_values=True,\n",
        "    do_sample=True,\n",
        "    max_length=50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "print(tokenizer.decode(outputs[1], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "RaPor2pQ_c66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ./tknzer.zip ./tokenizer/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpuVSplbluS_",
        "outputId": "181991cf-6efe-4036-f247-608c8a4d78db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: tokenizer/ (stored 0%)\n",
            "  adding: tokenizer/vocab.json (deflated 59%)\n",
            "  adding: tokenizer/merges.txt (deflated 57%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!zip -r ./gpt_train.zip ./ddGPT/"
      ],
      "metadata": {
        "id": "xL4CieKjl7zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ./gpt_model.zip ./ddGptModel/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XoC49FZl8jW",
        "outputId": "3459f0c0-0c97-492c-b0cf-e9b63835d8f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: ddGptModel/ (stored 0%)\n",
            "  adding: ddGptModel/pytorch_model.bin (deflated 10%)\n",
            "  adding: ddGptModel/training_args.bin (deflated 48%)\n",
            "  adding: ddGptModel/config.json (deflated 52%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load"
      ],
      "metadata": {
        "id": "wNdCZ5VATaAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip tknzerV2.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o349P1hCTbyO",
        "outputId": "cee54b83-dc3a-4e72-a19d-4cbe66438f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  tknzerV2.zip\n",
            "   creating: tokenizer/\n",
            "  inflating: tokenizer/vocab.json    \n",
            "  inflating: tokenizer/merges.txt    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gpt_modelV2.zip"
      ],
      "metadata": {
        "id": "JQkcqfoJTwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "233631c6-5aa5-469f-8cc0-8d549ba6a189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gpt_modelV2.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of gpt_modelV2.zip or\n",
            "        gpt_modelV2.zip.zip, and cannot find gpt_modelV2.zip.ZIP, period.\n"
          ]
        }
      ]
    }
  ]
}